{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parrot Predictions Courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Introduction\n",
    "There are planty of possible parameters possible to tune when fitting a XGBoost algorithm. The following notebook wraps them up, presenting possible choices and differences between interfaces (native and Scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "- `booster`=[**gbtree**|gblinear] - which booster to use\n",
    "- `silent`=[**0**|1] - 0 prints running messages, 1 means silent mode\n",
    "- `nthread` - number of threads to use. Maximum by default\n",
    "\n",
    "#### Tree Booster\n",
    "- `eta`=[0 .. **0.3** .. 1] - step shrinkage used in update to prevents overfitting. After each boosting step eta shrinks instance weights. Higher value makes learning process more conservative (slower learning),\n",
    "- `gamma`=[**0** .. ∞] - minimum loss reduction required to make a further partition on a leaf node of the tree. Higher value makes the algorithm more conservative,\n",
    "- `max_depth`=[1 .. **6** .. ∞] - maximum depth of each tree,\n",
    "- `min_child_weight`=[0 .. **1** .. ∞] - minimum sum of instance weight needed in a tree node. Further partitioning from that node is abandoned when a sum is not obtained. Higher value makes the algorithm more conservative,\n",
    "- `max_delta_step`=[**0** .. ∞] - maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value it can help making the update step more conservative,\n",
    "- `subsample`=[0,**1**] - subsample ratio of randomly selected training instances used to grow trees, lower value introduce randomness which might reduce variance,\n",
    "- `colsample_bytree`=[0,**1**] - subsample ratio of columns when construction each tree, lower value introduce randomness which might reduce variance,\n",
    "- `lambda`=[**1**] - L2 regularization term on weights,\n",
    "- `alpha`=[**0**] - L1 regularization term on weights\n",
    "\n",
    "#### Linear Booster\n",
    "- `alpha`=[**0**] - L1 regularization term on weights,\n",
    "- `lambda`=[**0**] - L2 regularization term on weights,\n",
    "- `lambda_bias`=[**0**] - L2 regularization term on bias\n",
    "\n",
    "### Learning task parameters\n",
    "- `objective`=[\n",
    "    - **reg:linear** (*linear regression*),\n",
    "    - reg:logistic (*logistic regression*),\n",
    "    - binary:logistic (*logistic regression for binary classification, outputs probability*),\n",
    "    - binary:logitraw (*logistic regression for binary classification, outputs score before logistic transformation*),\n",
    "    - count:poisson (*poisson regression for count data, outputs mean of poisson distribution*),\n",
    "    - multi:softmax (*do multiclass classification using softmax objective*),\n",
    "    - multi:softprob (*same as above but outputs probability for each class*),\n",
    "    - rank:pairwise (*execute ranking task by minimizing the pairwise loss*)\n",
    "  \n",
    "  ]\n",
    "- `base_score`=[**0.5**] - global bias. Initial prediction score for all instances,\n",
    "- `eval_metric`=[rmse|logloss|error|merror|mlogloss|auc|...] - evaluation metric. Default value will be assigned based on the objective. There is possibility of having custom metric,\n",
    "- `seed`=[**0**] - seed used for reproducibility\n",
    "\n",
    "### Others\n",
    "- `weights` ? TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
